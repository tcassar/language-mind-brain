# Plan

## Target Audience

- Technically literate audience with a wider interest in scientific topics.
- No expertise or background knowledge whatsoever in linguistics
- Probably haven't studied a subject from the humanities at anything above a
  GCSE level.

### Where to find

- HackerNews
- [Gwern](https://gwern.net/) (specifically the Psychology section)
- [Astral Codex Ten](https://www.astralcodexten.com/)

### Features from target audience

- Technical hook needed
- Proper references
- Lots of images, diagrams
- Informal tone

---

## Plan

## Intro (50 Words)

- Playing with my very young cousin, made incomprehensible babbling noises
- Reminded me of LLM responses in early training phases: complete
  gibberish.
- Made me question how humans learn language, and is it anything like
  LLMs?

## Language acquired in phases (120 Words)

- Consistent phases: reflexive vocalisation -> babbling -> words
- MLU increases with time, with the earliest multi-word utterances all
  being gramatically predictable. Shows clear internal structure is being
  acquired (or is there, if you ask a nativist)
- Interestingly, spoken languages are acquired at the same rate as signed
  languages: **modality independence**
- Exception: time to arrive at one word stage. This is a physiological
  limitation, as evidenced by bilingual signing/speaking children)

## Receptive Language Ability (Children's language learning inputs)

- Physiology in general undeniably plays an important role in child
  language development.
- Up until a year old, hearing children can tell the difference between
  sounds from other languages. After a year, this becomes much more
  difficult. (e.g. hindi dth vs d) TODO: have chatgpt do the phonetic
  alphabet.

## Nativism, Behaviourism, and LLMs

- Key assumption in understanding CLD: is there _help from the hardware?_
- Behaviourism (Watson): **no** - language is learned by children with no
  innate drive
- Chomsky, nativism: **yes** - language is too massive to learn purely
  through reinforcement! Children only ever get +ve information. This +
  grammatically correct two-word utterances shows some sort of innate
  _Language Acquisition Device_.

- LLMs could be an interesting counterpoint to this

  - "Weakly supervised": some labelled training data, mostly unsupervised
  - Same inputs as children (not even Child Directed Speech); only positive
    examples; no one would argue GPU silicon is innately designed for human
    language. LLMs seem to learn english fine! (probably has a better grasp
    on grammar rules than me!)

- However:
  - Does this tell us anything about CLD?
  - You wouldn't infer how a bird flies by studying an aeroplane...
