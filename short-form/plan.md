# Plan

## Target Audience

- Technically literate audience with a wider interest in scientific topics.
- No expertise or background knowledge whatsoever in linguistics
- Probably haven't studied a subject from the humanities at anything above a
  GCSE level.

### Where to find

- HackerNews
- [Gwern](https://gwern.net/) (specifically the Psychology section)
- [Astral Codex Ten](https://www.astralcodexten.com/)

### Features from target audience

- Technical hook needed
- Proper references
- Lots of images, diagrams
- Informal tone

---

## Plan

## Intro (50 Words)

- Building LLM; early in training, random noise.
- Reminded me of baby cousin making random noises when she started
  speaking.
- Made me curious: how do children learn languages? How different is it
  from LLMs?

## Language acquired in phases (120 Words)

- Reflexive vocalisation -> babbling -> words
- MLU increases with time, with the earliest multi-word utterances all
  being gramatically predictable.
- Interestingly, spoken languages are acquired at the same rate as signed
  languages (aside from one word stage: this is a physiological limitation,
  as evidenced by bilingual signing/speaking children)

## Receptive Language Ability (Children's language learning inputs)

- Physiology in general undeniably plays an important role in child
  language development.
- Up until a year old, hearing children can tell the difference between
  sounds from other languages. After a year, this becomes much more
  difficult. (e.g. hindi dth vs d) TODO: have chatgpt do the phonetic
  alphabet.

## Nativism, Behaviourism, and LLMs

- Key assumption in understanding CLD: is there _help from the hardware?_
- Behaviourism (Watson): **no** - language is learned by children with no
  innate drive
- Chomsky, nativism: **yes** - language is too massive to learn purely
  through reinforcement! Children only ever get +ve information. This +
  grammatically correct two-word utterances shows some sort of innate
  _Language Acquisition Device_.

- LLMs could be an interesting counterpoint to this

  - "Weakly supervised": some labelled training data, mostly unsupervised
  - Same inputs as children (not even Child Directed Speech); only positive
    examples; no one would argue GPU silicon is innately designed for human
    language. LLMs seem to learn english fine! (probably has a better grasp
    on grammar rules than me!)

- However:
  - Does this tell us anything about CLD?
  - You wouldn't infer how a bird flies by studying an aeroplane...
