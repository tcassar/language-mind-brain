@misc{tensorflow_word_embeddings,
    title = {Word Embeddings},
    howpublished = {\url{
                    https://www.tensorflow.org/text/tutorials/word_embeddings}},
    note = {Accessed: 2025-04-09},
    institution = {TensorFlow},
    year = {n.d.},
}

@inproceedings{bazhukov-etal-2024-models,
    title = "Of Models and Men: Probing Neural Networks for Agreement Attraction
             with Psycholinguistic Data",
    author = "Bazhukov, Maxim and Voloshina, Ekaterina and Pletenev, Sergey and
              Anisimov, Arseny and Serikov, Oleg and Toldova, Svetlana",
    editor = "Barak, Libby and Alikhani, Malihe",
    booktitle = "Proceedings of the 28th Conference on Computational Natural
                 Language Learning",
    month = nov,
    year = "2024",
    address = "Miami, FL, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.conll-1.22/",
    doi = "10.18653/v1/2024.conll-1.22",
    pages = "280--290",
    abstract = "Interpretability studies have played an important role in the
                field of NLP. They focus on the problems of how models encode
                information or, for instance, whether linguistic capabilities
                allow them to prefer grammatical sentences to ungrammatical.
                Recently, several studies examined whether the models demonstrate
                patterns similar to humans and whether they are sensitive to the
                phenomena of interference like humans' grammaticality judgements,
                including the phenomenon of agreement attraction.In this paper,
                we probe BERT and GPT models on the syntactic phenomenon of
                agreement attraction in Russian using the psycholinguistic data
                with syncretism. Working on the language with syncretism between
                some plural and singular forms allows us to differentiate between
                the effects of the surface form and of the underlying grammatical
                feature. Thus we can further investigate models' sensitivity to
                this phenomenon and examine if the patterns of their behaviour
                are similar to human patterns. Moreover, we suggest a new way of
                comparing models' and humans' responses via statistical testing.
                We show that there are some similarities between models' and
                humans' results, while GPT is somewhat more aligned with human
                responses than BERT. Finally, preliminary results suggest that
                surface form syncretism influences attraction, perhaps more so
                than grammatical form syncretism.",
}


@misc{3Blue1Brown_2024,
    author = {3Blue1Brown},
    title = {How Might LLMs Store Facts | DL7},
    year = {2024},
    month = aug,
    howpublished = {\url{https://www.youtube.com/watch?v=9-Jl0dxWQs8}},
    note = {YouTube video},
    language = {English},
}
